# -*- coding: utf-8 -*-
"""Unet denoising.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APDI_0NVBitRYly1my21usSzem4RqH43
"""

import os
import pandas as pd
import numpy as np

import PIL
import PIL.Image
import pathlib
import time
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import MaxPool2D
from tensorflow.keras.layers import UpSampling2D
from tensorflow.keras.layers import Concatenate
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Activation

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

import io
import random

print(tf.__version__)

# Define parameters

# CNN
conv_model      = "UNet" # choose model: UNet, Convnet, ResNet
#
# Param for UNet and Convnet
conv_filt       = 32        # filters on first layer
kernel_size     = 3
activation      = "relu"
padding         = "same"
pool_size       = (2, 2)

# TRAIN
batch_size      = 64 # 4
epochs          = 10       # Early stopping on, check callbacks
optimizer       = "Adam"
learning_rate   = 1e-4
validation_split = 0.1
train_eval_ratio = 0.8

# Continue training a model from checkpoint
mode_continue_training = False
if mode_continue_training == True:
    path_previous_model = os.path.join(path_results,
                            "kits19_200subj_data2k_UNet_32_noisepc5_model_best.h5")
    name_save = name_save + '_cont300it'

# Data augmentation
#
# central crop before applying random crop
central_fraction = 0.7

# IMAGE
img_width, img_height, img_channels = (256,256,1) # (64,64,1)
input_shape     = (img_width, img_height, img_channels)

# Reduced randomly number of data slices
mode_limited_data = True
if mode_limited_data == True:
    ds_train_size   = 1000 # number slices in training set
ds_test_size    = 100

# Percentage of additive noise
perc_noise      = 0.05

# PATHS
current_path = '/content/drive/MyDrive'

subpath_data_train = current_path+'/noisyaddphantom_train'

subpath_data_test  = current_path+'/noisyaddphantom_test'

# Create file list
#
# Train set
# tf.data.Dataset.from_tensor_slices with glob more efficient

fnames_train = os.listdir(subpath_data_train)
fnames_test = os.listdir(subpath_data_test)


# Datasets ds train and test
if mode_limited_data == False:
    ds_train_size  = len(fnames_train)
else:
    # Randomly select slices from the entire training set
    random.shuffle(fnames_train)

import imageio
N = 47
img = imageio.imread(current_path+'/noisyaddphantom_train/'+fnames_train[N])
mask = imageio.imread(current_path+'/idealphantom/ideal.png')
#mask = mask[:,:,0]
print(img.shape)
fig, arr = plt.subplots(1, 2, figsize=(14, 10))
arr[0].imshow(img)
arr[0].set_title('Noisy')
arr[1].imshow(mask)
arr[1].set_title('ideal')

# Train set
fnames_train =  [current_path+'/noisyaddphantom_train/'+i for i in fnames_train]
filelist_train_ds = tf.data.Dataset.list_files(fnames_train[:ds_train_size])

# Check train ds: display images (can simply iterate over items in the data set)
# filelist_ds     = tf.data.Dataset.list_files(str(data_dir/'*/*'))
if 0:
  for a in filelist_train_ds.take(3):
      fname = a.numpy().decode("utf-8") # not in TF 1.4
      print(fname)
      display(PIL.Image.open(fnameds_train_size))
print(ds_train_size)
# Split train and eval
ds_train = filelist_train_ds.take(int(ds_train_size*train_eval_ratio))
ds_eval = filelist_train_ds.skip(int(ds_train_size*train_eval_ratio))

print("Train data num: ", ds_train.cardinality().numpy())
print("Eval data num: ", ds_eval.cardinality().numpy())

# Test set
fnames_test =  [current_path+'/noisyaddphantom_test/'+i for i in fnames_test]
filelist_test_ds = tf.data.Dataset.list_files(fnames_test[:ds_test_size])
ds_test = filelist_test_ds.take(ds_test_size)
print("Test data num: ", ds_test.cardinality().numpy())

def flip(x: tf.Tensor) -> tf.Tensor:
    """Flip augmentation
    Args:
        x: Image to flip
    Returns:
        Augmented image
    """
    #x = tf.image.random_flip_left_right(x)
    #x = tf.image.random_flip_up_down(x)

    return x

def random_crop(img):
    #img = tf.image.central_crop(
        #img, central_fraction)
    #img = tf.image.random_crop(
           # img, size=[img_height, img_width, 1])

    return img

# Additive noise
def add_gaussian_noise(img):
    #img_noisy = img + perc_noise*np.random.normal(0,1,img.shape)
    img_noisy = img + perc_noise*tf.random.normal(img.shape,mean=0,stddev=1)
    return img_noisy

# Load and process image
def process_img(img):
    img = tf.image.decode_png(img, img_channels)
    print("dd",img_channels)
    # convert unit8 tensor to floats in range [0,1]
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, [img_width, img_height])
    return img

def get_process_target(file_path: tf.Tensor):
    img = tf.io.read_file(file_path)
    img = process_img(img)
    return img
#Transform data
# Load, normalize, data augmentation (random crop, horizontal flip),
# corrupt with additive Gaussian noise
map_ops     = [get_process_target, flip, random_crop] # Select data aug ops (flip, random_crop)
for f in map_ops:
    print(f)

# TRAIN
for f in map_ops:

    ds_train = ds_train.map(lambda x: f(x),
        num_parallel_calls=tf.data.AUTOTUNE) # num_parallel_calls for multithreading
"""
for a in ds_train.take(2):
    plt.imshow(a.numpy()[:,:,0])
    plt.show()
"""
mask = tf. image. resize(mask,[256,256])
mask = tf.reshape(mask[:,:,0], (256,256,1))
ds_train = ds_train.map(  # return tuple (noisy, target) for training
        lambda x: (add_gaussian_noise(x), mask),
        num_parallel_calls = tf.data.AUTOTUNE)
print(ds_train)
# EVAL
for f in map_ops:
    ds_eval = ds_eval.map(lambda x: f(x),
        num_parallel_calls=tf.data.AUTOTUNE)
ds_eval = ds_eval.map(
        lambda x: (add_gaussian_noise(x), mask),
        num_parallel_calls = tf.data.AUTOTUNE)
print(ds_eval)
# TEST
for f in map_ops:
    ds_test = ds_test.map(lambda x: f(x),
        num_parallel_calls=tf.data.AUTOTUNE)
ds_test = ds_test.map(
        lambda x: (add_gaussian_noise(x), mask),
        num_parallel_calls = tf.data.AUTOTUNE)
print(ds_test)
# Buffer size greater than or equal to the full size of the dataset
buffer_size_train = ds_train.cardinality().numpy()
buffer_size_eval = ds_eval.cardinality().numpy()
print(buffer_size_train)

ds_train_batched = ds_train.cache().batch(batch_size)  #
ds_train_batched = ds_train_batched.shuffle(buffer_size=buffer_size_train,
                                            reshuffle_each_iteration=True,
                                            seed = 50)
ds_train_batched = ds_train_batched.prefetch(tf.data.experimental.AUTOTUNE)
#
ds_eval_batched = ds_eval.cache().batch(batch_size)
ds_eval_batched = ds_eval_batched.shuffle(buffer_size=buffer_size_eval,
                                          reshuffle_each_iteration=True,
                                            seed = 50)
ds_eval_batched = ds_eval_batched.prefetch(tf.data.experimental.AUTOTUNE)

print(batch_size)

def get_model_unet(input_shape = [28,28,1],
                             conv_filt=32,
                             kernel_size=3,
                             activation="relu",
                             padding="same",
                             pool_size=pool_size):
    conv_args = {"activation": activation,
                 "padding": padding,
                 "kernel_size": kernel_size}
    input_ = Input(shape=input_shape)
    conv1 = Conv2D(filters=conv_filt, **conv_args)(input_)
    conv2 = Conv2D(filters=conv_filt, **conv_args)(conv1)
    pool1 = MaxPool2D(pool_size=pool_size)(conv2)
    #
    conv3 = Conv2D(filters=2*conv_filt, **conv_args)(pool1)
    conv4 = Conv2D(filters=2*conv_filt, **conv_args)(conv3)
    pool2 = MaxPool2D(pool_size=pool_size)(conv4)
    #
    conv5 = Conv2D(filters=4*conv_filt, **conv_args)(pool2)
    conv6 = Conv2D(filters=2*conv_filt, **conv_args)(conv5)
    up1 = UpSampling2D(size=(2,2))(conv6)
    #
    conc1 = Concatenate()([conv4, up1])
    #
    conv7 = Conv2D(filters=2*conv_filt, **conv_args)(conc1)
    conv8 = Conv2D(filters=conv_filt, **conv_args)(conv7)
    up2 = UpSampling2D(size=(2,2))(conv8)
    #
    conc2 = Concatenate()([conv2, up2])
    #
    conv9 = Conv2D(filters=conv_filt, **conv_args)(conc2)
    conv10 = Conv2D(filters=conv_filt, **conv_args)(conv9)
    #
    output = Conv2D(filters=1, kernel_size=1, activation=None)(conv10)
    #
    model = keras.Model(inputs=[input_], outputs=[output])
    return model

if mode_continue_training == True:
    # Continue training a model from checkpoint
    #
    # Load previous model
    model = keras.models.load_model(path_previous_model)

else:
    # Define model
    model   = get_model_unet(
        input_shape=input_shape,
        conv_filt=conv_filt,
        kernel_size=kernel_size,
        activation=activation,
        padding=padding,
        pool_size=pool_size)

    loss_fn = keras.losses.MeanSquaredError()
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer,
              loss=loss_fn,
              metrics=["mse"])

model.summary()
# model.layers

# TRAIN
#
# Fit the model
history = model.fit(ds_train_batched,
                    epochs=epochs,
                    validation_data=ds_eval_batched)
                    #callbacks=callbacks # uncomment to write the callbacks (eg. tensorboard)

# SAVE MODEL
path_results = os.path.join(current_path, 'Results')
if os.path.exists(path_results) is False:
    os.mkdir(path_results)
name_save = 'Kits19_subset_UNet'
model.save(os.path.join(path_results, name_save + "_model_last.h5"))

pd.DataFrame(history.history).plot(figsize=[8, 5], logy=True)
plt.show()



def get_imgs_from_dataset(ds_test, ds_test_size):
    # Take images from data set ds_test: (data_test, data_test_noisy)
    data_test = []
    data_test_noisy = []
    count = 0
    for a_n, a in ds_test.take(ds_test_size):
        data_test_this = a.numpy()
        data_test_noisy_this = a_n.numpy()
        if count == 0:
            data_test = data_test_this
            data_test_noisy = data_test_this
            count = 1
        else:
            data_test = np.append(data_test, data_test_this, axis=2)
            data_test_noisy = np.append(data_test_noisy, data_test_noisy_this, axis=2)
    data_test = np.transpose(data_test, (2,0,1))
    data_test_noisy = np.transpose(data_test_noisy, (2,0,1))
    data_test = data_test[:,:,:,np.newaxis]
    data_test_noisy = data_test_noisy[:,:,:,np.newaxis]
    return data_test, data_test_noisy

# Test data
data_test, data_test_noisy = get_imgs_from_dataset(ds_test, ds_test_size)

# IMAGE VISUALIZATION
def displaySlices(X_Tensor, titles, figsize=(12,4), nameSave = []):
    # displaySlices(X_Tensor, titles)
    # displaySlices(X_Tensor, titles, figsize=(12,6), nameSave = 'name.png')
    #
    # Display slices (target, noisy image and prior)
    #
    # Inputs:
    #
    # X_Tensor = tuple of 2D arrays.
    # X_Tensor =  (X_batch_train[0,:,:,0],S_batch_train[0,:,:,0],S_batch_pr_train[0,:,:,0])
    # titles    = ('Target','Noisy','Prior')
    num_dim    = len(X_Tensor)
    fig, axarr = plt.subplots(1, num_dim, figsize=figsize)
    fig.tight_layout()
    for i in range(num_dim):
        img0 = axarr[i].imshow(X_Tensor[i],cmap='gray')
        divider = make_axes_locatable(axarr[i])
        cax = divider.append_axes("right", size="7%", pad=0.05)
        plt.colorbar(img0, cax = cax)
        axarr[i].set_title(titles[i])
        axarr[i].axis('off')
    plt.show()
    if nameSave:
        fig.savefig(nameSave, dpi = 300, bbox_inches='tight')
    return fig

data_pred = model.predict(data_test_noisy)
data_pred_err = np.mean(100*np.linalg.norm(data_test[:,:,:,0]-
                data_pred[:,:,:,0], axis=(1, 2))/np.linalg.norm(data_test[:,:,:,0], axis=(1, 2)))
print('Error %.1f' % data_pred_err)

# Display result
print('Example of restored images')
for i in range(5):
    displaySlices([data_test[i,:,:,0], data_test_noisy[i,:,:,0],
                   data_pred[i,:,:,0]],
               ['Raw image', 'Noisy image', 'Denoised image'],
               figsize=(12,4),
               nameSave=os.path.join(path_results,
                            name_save+'_test_ex'+str(i)+'.png'))

